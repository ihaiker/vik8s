namespace: kube-system;

secret calico-etcd-secrets Opaque {
    # Populate the following with etcd TLS configuration if desired, but leave blank if
    # not using TLS for etcd.
    # The keys below should be uncommented and the values populated with the base64
    # encoded contents of each file that would be associated with the TLS data.
    # Example command for encoding a file contents: cat <file> | base64 -w 0

    {{if .Etcd.TLS}}
    etcd-key: "{{ .Etcd.KeyBase64 }}";
    etcd-cert: "{{ .Etcd.CertBase64 }}";
    etcd-ca: "{{ .Etcd.CaBase64 }}";
    {{end}}
}

# Source: calico/templates/calico-config.yaml
# This ConfigMap is used to configure a self-hosted Calico installation.
configmap calico-config {
    # Configure this with the location of your etcd cluster.
    etcd_endpoints: "{{ .Etcd.EndpointsUrl }}";

    # If you're using TLS enabled etcd uncomment the following.
    # You must also populate the Secret below with these files.
    etcd_ca: "{{ if .Etcd.TLS }}/calico-secrets/etcd-ca{{ end }}";
    etcd_cert: "{{ if .Etcd.TLS }}/calico-secrets/etcd-cert{{ end }}";
    etcd_key: "{{ if .Etcd.TLS }}/calico-secrets/etcd-key{{ end }}";

    # Typha is disabled.
    typha_service_name: "none";

    # Configure the backend to use.
    calico_backend: "bird";

    # Configure the MTU to use for workload interfaces and the
    # tunnels.  For IPIP, set to your network MTU - 20; for VXLAN
    # set to your network MTU - 50.
    veth_mtu: "'1440'";

    # The CNI network configuration to install on each node.  The special
    # values in this config will be automatically populated.
    cni_network_config: '
    {
        "name": "k8s-pod-network",
        "cniVersion": "0.3.1",
        "plugins": [
            {
                "type": "calico",
                "log_level": "warn",
                "etcd_endpoints": "__ETCD_ENDPOINTS__",
                "etcd_key_file": "__ETCD_KEY_FILE__",
                "etcd_cert_file": "__ETCD_CERT_FILE__",
                "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
                "mtu": __CNI_MTU__,
                "ipam": {
                    "type": "calico-ipam"
                },
                "policy": {
                    "type": "k8s"
                },
                "kubernetes": {
                    "kubeconfig": "__KUBECONFIG_FILEPATH__"
                }
            },
            {
                "type": "portmap",
                "snat": true,
                "capabilities": {
                    "portMappings": true
                }
            },
            {
                "type": "bandwidth",
                "capabilities": {
                    "bandwidth": true
                }
            }
        ]
    }
    ';
}

# Include a clusterrole for the kube-controllers component,
# and bind it to the calico-kube-controllers serviceaccount.
ClusterRole:rbac.authorization.k8s.io/v1 calico-kube-controllers {
    # Pods are monitored for changing labels.
    # The node controller monitors Kubernetes nodes.
    # Namespace and serviceaccount labels are used for policy.
    rules {
        apiGroups: "";
        resources: pods nodes namespaces serviceaccounts;
        verbs: watch list get;
    }
    # Watch for changes to Kubernetes NetworkPolicies.
    rules {
        apiGroups: "networking.k8s.io";
        resources: networkpolicies;
        verbs: watch list;
    }
}

ClusterRoleBinding:rbac.authorization.k8s.io/v1 calico-kube-controllers {
    roleRef {
        apiGroup: rbac.authorization.k8s.io;
        kind: ClusterRole;
        name: calico-kube-controllers;
    }
    subjects {
        kind: ServiceAccount;
        name: calico-kube-controllers;
        namespace: kube-system;
    }
}


# Include a clusterrole for the calico-node DaemonSet,
# and bind it to the calico-node serviceaccount.
ClusterRole:rbac.authorization.k8s.io/v1 calico-node {
    # The CNI plugin needs to get pods, nodes, and namespaces.
    rules {
        apiGroups: "";
        resources: pods nodes namespaces;
        verbs: get;
    }
    rules {
        apiGroups: "";
        resources: endpoints services;
        verbs: watch list;
    }
    # Pod CIDR auto-detection on kubeadm needs access to config maps.
    rules {
        apiGroups: "";
        resources: configmaps;
        verbs: get;
    }
    rules {
        apiGroups: "";
        resources: "nodes/status";
        verbs: patch;
    }
}

ClusterRoleBinding:rbac.authorization.k8s.io/v1 calico-node {
    roleRef {
        apiGroup: rbac.authorization.k8s.io;
        kind: ClusterRole;
        name: calico-node;
    }
    subjects {
        kind: ServiceAccount;
        name: calico-node;
        namespace: kube-system;
    }
}

DaemonSet calico-node {
    hostNetwork: true;
    nodeSelector {
        kubernetes.io/os: linux;
    }
    updateStrategy {
        type: RollingUpdate;
        rollingUpdate: maxUnavailable=1;
    }
    template {
        metadata {
            annotations: {
                scheduler.alpha.kubernetes.io/critical-pod: '';
            }
        }
    }
    tolerations effect=NoSchedule operator=Exists;
    tolerations key=CriticalAddonsOnly operator=Exists;
    tolerations effect=NoExecute operator=Exists;

    serviceAccountName: calico-node;
    terminationGracePeriodSeconds: 0;
    priorityClassName: system-node-critical;

    # This container installs the CNI binaries
    # and CNI network config file on each node.
    initContainer install-cni "{{.Repo}}calico/cni:{{.Version}}" {
        securityContext privileged=true;
        command: "/install-cni.sh";
        envs {
            CNI_CONF_NAME "10-calico.conflist";
            CNI_NETWORK_CONFIG configMap calico-config cni_network_config;
            ETCD_ENDPOINTS configMap calico-config etcd_endpoints;
            CNI_MTU configMap calico-config veth_mtu;
            SLEEP false;
        }
        mount hostPath:cni-bin-dir /opt/cni/bin:/host/opt/cni/bin;
        mount hostPath:cni-net-dir /etc/cni/net.d:/host/etc/cni/net.d;
        mount secret:etcd-certs:calico-etcd-secrets /calico-secrets 0400;
    }

    initContainer flexvol-driver "{{.Repo}}calico/pod2daemon-flexvol:{{.Version}}" {
        securityContext privileged=true;
        mount hostPath:flexvol-driver-host
            /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds:/host/driver
            DirectoryOrCreate;
    }

    container calico-node "{{.Repo}}calico/node:{{.Version}}" {
        securityContext privileged=true;
        envs {
            ETCD_ENDPOINTS configMap calico-config etcd_endpoints;
            ETCD_CA_CERT_FILE configMap calico-config etcd_ca;
            ETCD_KEY_FILE configMap calico-config etcd_key;
            ETCD_CERT_FILE configMap calico-config etcd_cert;
            CALICO_K8S_NODE_REF field spec.nodeName;
            CALICO_NETWORKING_BACKEND configMap calico-config calico_backend;
            CLUSTER_TYPE "k8s,bgp";
            IP "autodetect";
            IP_AUTODETECTION_METHOD "interface={{ .Interface }}";
            CALICO_IPV4POOL_IPIP "{{if not .IPIP }}Off{{else}}Always{{end}}";
            FELIX_IPINIPMTU configMap calico-config veth_mtu;
            FELIX_VXLANMTU configMap calico-config veth_mtu;

            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            CALICO_IPV4POOL_CIDR "{{ .CIDR }}";

            CALICO_DISABLE_FILE_LOGGING "true";
            # Set Felix endpoint to host default action to ACCEPT.
            FELIX_DEFAULTENDPOINTTOHOSTACTION "ACCEPT";
            # Disable IPv6 on Kubernetes.
            FELIX_IPV6SUPPORT "false";
            # Set Felix logging to "info"
            FELIX_LOGSEVERITYSCREEN "warn";
            FELIX_HEALTHENABLED "true";
        }
        resources {
            requests cpu=250m;
        }

        livenessProbe {
            exec {
                command: "/bin/calico-node" "-felix-live" "-bird-live";
            }
            periodSeconds: 10;
            initialDelaySeconds: 10;
            failureThreshold: 6;
        }
        readinessProbe {
            exec {
                command: "/bin/calico-node" "-felix-ready" "-bird-ready";
            }
            periodSeconds: 10;
        }

        mounts {
            hostPath:lib-modules /lib/modules {
                readOnly true;
            }
            hostPath:xtables-lock /run/xtables.lock FileOrCreate;
            hostPath:var-run-calico /var/run/calico;
            hostPath:var-lib-calico /var/lib/calico;
            hostPath:policysync /var/run/nodeagent DirectoryOrCreate;
            secret:etcd-certs:calico-etcd-secrets /calico-secrets 0400;
        }
    }
}

ServiceAccount:v1 calico-node;

Deployment calico-kube-controllers {
    replicas: 1;
    strategy type=Recreate;

    template {
        metadata {
            annotations scheduler.alpha.kubernetes.io/critical-pod "";
        }
    }
    nodeSelector {
        kubernetes.io/os: linux;
    }

    tolerations key:CriticalAddonsOnly operator:Exists;
    tolerations key:node-role.kubernetes.io/master effect:NoSchedule;
    serviceAccountName: calico-kube-controllers;
    priorityClassName: system-cluster-critical;
    # The controllers must run in the host network namespace so that
    # it isn't governed by policy that would prevent it from working.
    hostNetwork: true;

    container calico-kube-controllers "{{.Repo}}calico/kube-controllers:{{.Version}}" {
        envs {
            # The location of the etcd cluster.
            ETCD_ENDPOINTS configMap calico-config etcd_endpoints;
            # Location of the CA certificate for etcd.
            ETCD_CA_CERT_FILE configMap calico-config etcd_ca;
            # Location of the client key for etcd.
            ETCD_KEY_FILE configMap calico-config etcd_key;
            # Location of the client certificate for etcd.
            ETCD_CERT_FILE configMap calico-config etcd_cert;
            # Choose which controllers to run.
            ENABLED_CONTROLLERS "policy,namespace,serviceaccount,workloadendpoint,node";
        }
        readinessProbe {
            exec {
                command: "/usr/bin/check-status" "-r";
            }
        }
        mount secret:etcd-certs:calico-etcd-secrets /calico-secrets 0400;
    }
}

ServiceAccount:v1 calico-kube-controllers;