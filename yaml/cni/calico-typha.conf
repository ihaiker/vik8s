namespace: kube-system;

# This ConfigMap is used to configure a self-hosted Calico installation.
ConfigMap calico-config {

    # You must set a non-zero value for Typha replicas below.
    typha_service_name: "calico-typha";

    # Configure the backend to use.
    calico_backend: "bird";

    # Configure the MTU to use for workload interfaces and the
    # tunnels.  For IPIP, set to your network MTU - 20; for VXLAN
    # set to your network MTU - 50.
    veth_mtu: "'{{.MTU}}'";

    # The CNI network configuration to install on each node.  The special
    # values in this config will be automatically populated.
    cni_network_config: '
    {
      "name": "k8s-pod-network",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "calico",
          "log_level": "warn",
          "datastore_type": "kubernetes",
          "nodename": "__KUBERNETES_NODE_NAME__",
          "mtu": __CNI_MTU__,
          "ipam": {
              "type": "calico-ipam"
          },
          "policy": {
              "type": "k8s"
          },
          "kubernetes": {
              "kubeconfig": "__KUBECONFIG_FILEPATH__"
          }
        },
        {
          "type": "portmap",
          "snat": true,
          "capabilities": {"portMappings": true}
        },
        {
          "type": "bandwidth",
          "capabilities": {"bandwidth": true}
        }
      ]
    }
  ';
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 bgpconfigurations.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: BGPConfiguration;
        plural: bgpconfigurations;
        singular: bgpconfiguration;
    }

}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 bgppeers.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: BGPPeer;
        plural: bgppeers;
        singular: bgppeer;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 blockaffinities.crd.projectcalico.org {

    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: BlockAffinity;
        plural: blockaffinities;
        singular: blockaffinity;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 clusterinformations.crd.projectcalico.org {

    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: ClusterInformation;
        plural: clusterinformations;
        singular: clusterinformation;
    }

}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 felixconfigurations.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: FelixConfiguration;
        plural: felixconfigurations;
        singular: felixconfiguration;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 globalnetworkpolicies.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: GlobalNetworkPolicy;
        plural: globalnetworkpolicies;
        singular: globalnetworkpolicy;
        shortNames: gnp;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 globalnetworksets.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: GlobalNetworkSet;
        plural: globalnetworksets;
        singular: globalnetworkset;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 hostendpoints.crd.projectcalico.org {

    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: HostEndpoint;
        plural: hostendpoints;
        singular: hostendpoint;
    }

}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 ipamblocks.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: IPAMBlock;
        plural: ipamblocks;
        singular: ipamblock;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 ipamconfigs.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: IPAMConfig ;
        plural: ipamconfigs;
        singular: ipamconfig;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 ipamhandles.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: IPAMHandle;
        plural: ipamhandles;
        singular: ipamhandle;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 ippools.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: IPPool;
        plural: ippools;
        singular: ippool;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 kubecontrollersconfigurations.crd.projectcalico.org {
    scope: Cluster;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: KubeControllersConfiguration;
        plural: kubecontrollersconfigurations;
        singular: kubecontrollersconfiguration;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 networkpolicies.crd.projectcalico.org {
    scope: Namespaced;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: NetworkPolicy;
        plural: networkpolicies;
        singular: networkpolicy;
    }
}

CustomResourceDefinition:apiextensions.k8s.io/v1beta1 networksets.crd.projectcalico.org {
    scope: Namespaced;
    group: crd.projectcalico.org;
    version: v1;
    names {
        kind: NetworkSet;
        plural: networksets;
        singular: networkset;
    }
}


# Include a clusterrole for the kube-controllers component,
# and bind it to the calico-kube-controllers serviceaccount.
ClusterRole:rbac.authorization.k8s.io/v1 calico-kube-controllers {
    # Nodes are watched to monitor for deletions.
    rules {
        apiGroups: "";
        resources: nodes;
        verbs: watch list get;
    }
    # Pods are queried to check for existence.
    rules {
        apiGroups: "";
        resources: pods;
        verbs: get;
    }
    # IPAM resources are manipulated when nodes are deleted.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: ippools;
        verbs: list;
    }
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: blockaffinities ipamblocks ipamhandles;
        verbs: get list create update delete;
    }
    # kube-controllers manages hostendpoints.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: hostendpoints;
        verbs: get list create update delete;
    }
    # Needs access to update clusterinformations.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: clusterinformations;
        verbs: get create update;
    }
    # KubeControllersConfiguration is where it gets its config
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: kubecontrollersconfigurations;
        verbs: get create update watch;
    }
}

ClusterRoleBinding:rbac.authorization.k8s.io/v1 calico-kube-controllers {
    roleRef {
        apiGroup: rbac.authorization.k8s.io;
        kind: ClusterRole;
        name: calico-kube-controllers;
    }
    subjects {
        kind: ServiceAccount;
        name: calico-kube-controllers;
        namespace: kube-system;
    }
}

# Include a clusterrole for the calico-node DaemonSet,
# and bind it to the calico-node serviceaccount.
ClusterRole:rbac.authorization.k8s.io/v1 calico-node {
    # The CNI plugin needs to get pods, nodes, and namespaces.
    rules {
        apiGroups: "";
        resources: pods nodes namespaces;
        verbs: get;
    }
    rules {
        apiGroups: "";
        resources: endpoints services;
        verbs: watch list get;
    }
    rules {
        apiGroups: "";
        resources: configmaps;
        verbs: get;
    }
    rules {
        apiGroups: "";
        resources: "nodes/status";
        verbs: patch update;
    }
    rules {
        apiGroups: "networking.k8s.io";
        resources: networkpolicies;
        verbs: watch list;
    }
    # Used by Calico for policy information.
    rules {
        apiGroups: "";
        resources: pods namespaces serviceaccounts;
        verbs: list watch;
    }
    # The CNI plugin patches pods/status.
    rules {
        apiGroups: "";
        resources: "pods/status";
        verbs: patch;
    }

    # Calico monitors various CRDs for config.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: globalfelixconfigs felixconfigurations bgppeers globalbgpconfigs bgpconfigurations
            ippools ipamblocks globalnetworkpolicies globalnetworksets networkpolicies
            networksets clusterinformations hostendpoints blockaffinities;
        verbs: get list watch;
    }
    # Calico must create and update some CRDs on startup.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: ippools felixconfigurations clusterinformations;
        verbs: create update;
    }
    # Calico stores some configuration information on the node.
    rules {
        apiGroups: "";
        resources: nodes;
        verbs: get list watch;
    }
    # These permissions are only requried for upgrade from v2.6, and can
    # be removed after upgrade or on fresh installations.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: bgpconfigurations bgppeers;
        verbs: create update;
    }
    # These permissions are required for Calico CNI to perform IPAM allocations.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: blockaffinities ipamblocks ipamhandles;
        verbs: get list create update delete;
    }
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: ipamconfigs;
        verbs: get;
    }
    # Block affinities must also be watchable by confd for route aggregation.
    rules {
        apiGroups: "crd.projectcalico.org";
        resources: blockaffinities;
        verbs: watch;
    }
    # The Calico IPAM migration needs to get daemonsets. These permissions can be
    # removed if not upgrading from an installation using host-local IPAM.
    rules {
        apiGroups: "apps";
        resources: daemonsets;
        verbs: get;
    }
}


ClusterRoleBinding:rbac.authorization.k8s.io/v1 calico-node {
    roleRef {
        apiGroup: rbac.authorization.k8s.io;
        kind: ClusterRole;
        name: calico-node;
    }
    subjects {
        kind: ServiceAccount;
        name: calico-node;
        namespace: kube-system;
    }
}

# This manifest creates a Service, which will be backed by Calico's Typha daemon.
# Typha sits in between Felix and the API server, reducing Calico's load on the API server.
service deployment:calico-typha calico-typha {
    ports {
        calico-typha calico-typha:5473/TCP;
    }
}

deployment calico-typha {
    replicas: "{{.TyphaReplicas}}";
    revisionHistoryLimit: 2;

    nodeSelector {
        kubernetes.io/os: linux;
    }
    hostNetwork: true;
    tolerations: key:CriticalAddonsOnly operator:Exists;

    serviceAccountName: calico-node;
    priorityClassName: system-cluster-critical;
    securityContext: fsGroup=65534;

    container calico-typha "{{.Repo}}calico/typha:{{.Version}}" {
        port calico-typha 5473/TCP;
        livenessProbe {
            httpGet {
                path: /liveness;
                port: 9098;
                host: localhost;
            }
            periodSeconds: 30;
            initialDelaySeconds: 30;
        }
        securityContext {
            runAsNonRoot: true;
            allowPrivilegeEscalation: false;
        }
        readinessProbe {
            httpGet {
                path: /readiness;
                port: 9098;
                host: localhost;
            }
            periodSeconds: 10;
        }
        envs {
            # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
            TYPHA_LOGSEVERITYSCREEN "info";
            # Disable logging to file and syslog since those don't make sense in Kubernetes.
            TYPHA_LOGFILEPATH "none";
            TYPHA_LOGSEVERITYSYS "none";

            # Monitor the Kubernetes API to find the number of running instances and rebalance connections.
            TYPHA_CONNECTIONREBALANCINGMODE: "kubernetes";
            TYPHA_DATASTORETYPE: "kubernetes";
            TYPHA_HEALTHENABLED: "true";
            {{if .TyphaPrometheus}}
            # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
            # this opens a port on the host, which may need to be secured.
            TYPHA_PROMETHEUSMETRICSENABLED: "true";
            TYPHA_PROMETHEUSMETRICSPORT: "9093";
            {{end}}
        }
    }
}

# This manifest creates a Pod Disruption Budget for Typha to allow K8s Cluster Autoscaler to evict
PodDisruptionBudget:policy/v1beta1 calico-typha {
    maxUnavailable: 1;
    selector {
        matchLabels {
            k8s-app: calico-typha;
        }
    }
}

DaemonSet calico-node {

    hostNetwork: true;
    serviceAccountName: calico-node;
    terminationGracePeriodSeconds: 0;
    priorityClassName: system-node-critical;

    nodeSelector {
        kubernetes.io/os: linux;
    }
    tolerations: effect=NoSchedule operator=Exists;
    tolerations: key=CriticalAddonsOnly operator=Exists;
    tolerations: effect=NoExecute operator=Exists;

    updateStrategy {
        type: RollingUpdate;
        rollingUpdate: maxUnavailable=1;
    }

    template {
        metadata {
            # This, along with the CriticalAddonsOnly toleration below,
            # marks the pod as a critical add-on, ensuring it gets
            # priority scheduling and that its resources are reserved
            # if it ever gets evicted.
            annotations {
                scheduler.alpha.kubernetes.io/critical-pod: '';
            }
        }
    }

    initContainers set-networkmanager "{{.Repo}}calico/cni:{{.Version}}" IfNotPresent {
        command: "/bin/sh" "-c"
        "echo -e '[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*' > /etc/NetworkManager/conf.d/calico.conf";
        securityContext privileged=true;
        mount hostPath:network-manager-dir /etc/NetworkManager/conf.d;
    }

    # This container performs upgrade from host-local IPAM to calico-ipam.
    # It can be deleted if this is a fresh installation, or if you have already
    # upgraded to use calico-ipam.
    initContainers upgrade-ipam "{{.Repo}}calico/cni:{{.Version}}" {
        command: "/opt/cni/bin/calico-ipam" "-upgrade";
        envs {
            KUBERNETES_NODE_NAME field spec.nodeName;
            CALICO_NETWORKING_BACKEND configMap calico-config calico_backend;
        }
        securityContext: privileged=true;
        mounts {
            hostPath:host-local-net-dir /var/lib/cni/networks;
            hostPath:cni-bin-dir /opt/cni/bin:/host/opt/cni/bin;
        }
    }

    # This container installs the CNI binaries
    # and CNI network config file on each node.
    initContainers install-cni "{{.Repo}}calico/cni:{{.Version}}" {
        command: "/install-cni.sh";
        envs {
            # Name of the CNI config file to create.
            CNI_CONF_NAME: "10-calico.conflist";
            # The CNI network config to install on each node.
            CNI_NETWORK_CONFIG configMap calico-config cni_network_config;
            # Set the hostname based on the k8s node name.
            KUBERNETES_NODE_NAME field spec.nodeName;
            # CNI MTU Config variable
            CNI_MTU configMap calico-config veth_mtu;
            # Prevents the container from sleeping forever.
            SLEEP "false";
        }
        mounts {
            hostPath:cni-bin-dir /opt/cni/bin:/host/opt/cni/bin;
            hostPath:cni-net-dir /etc/cni/net.d:/host/etc/cni/net.d;
        }
        securityContext: privileged=true;
    }

    # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
    # to communicate with Felix over the Policy Sync API.
    initContainers flexvol-driver "{{.Repo}}calico/pod2daemon-flexvol:{{.Version}}" {
        securityContext: privileged=true;
        mount hostPath:flexvol-driver-host
            /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds:/host/driver
            DirectoryOrCreate;
    }

    container calico-node "{{.Repo}}calico/node:{{.Version}}" {
        envs {
            # Use Kubernetes API as the backing datastore.
            DATASTORE_TYPE "kubernetes";
            # Typha support: controlled by the ConfigMap.
            FELIX_TYPHAK8SSERVICENAME configMap calico-config typha_service_name;
            # Wait for the datastore.
            WAIT_FOR_DATASTORE "true";
            # Set based on the k8s node name.
            NODENAME field spec.nodeName;
            # Choose the backend to use.
            CALICO_NETWORKING_BACKEND configMap calico-config calico_backend;
            # Cluster type to identify the deployment type
            CLUSTER_TYPE "k8s,bgp";
            # Auto-detect the BGP IP address.
            IP: "autodetect";
            IP_AUTODETECTION_METHOD "interface={{ .Interface }}";
            # Enable IPIP
            CALICO_IPV4POOL_IPIP "{{if not .IPIP }}Off{{else}}Always{{end}}";
            # Set MTU for tunnel device used if ipip is enabled
            FELIX_IPINIPMTU configMap calico-config veth_mtu;
            # Set MTU for the VXLAN tunnel device.
            FELIX_VXLANMTU configMap calico-config veth_mtu;

            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            CALICO_IPV4POOL_CIDR "{{ .CIDR }}";
            CALICO_DISABLE_FILE_LOGGING "true";
            # Set Felix endpoint to host default action to ACCEPT.
            FELIX_DEFAULTENDPOINTTOHOSTACTION "ACCEPT";
            # Disable IPv6 on Kubernetes.
            FELIX_IPV6SUPPORT "false";
            FELIX_LOGSEVERITYSCREEN "warn";
            FELIX_HEALTHENABLED "true";
        }
        securityContext privileged=true;

        resources requests {
            cpu: 250m;
        }
        livenessProbe {
            exec {
                command: "/bin/calico-node" "-felix-live" "-bird-live";
            }
            periodSeconds: 10;
            initialDelaySeconds: 10;
            failureThreshold: 6;
        }
        readinessProbe {
            exec {
                command: "/bin/calico-node" "-felix-ready" "-bird-ready";
            }
            periodSeconds: 10;
        }
    }
}


ServiceAccount:v1 calico-node;

Deployment calico-kube-controllers {
    strategy type=Recreate;
    template {
        metadata {
            annotations {
                scheduler.alpha.kubernetes.io/critical-pod: '';
            }
        }
    }
    nodeSelector {
        kubernetes.io/os: linux;
    }
    tolerations: key=CriticalAddonsOnly operator=Exists;
    tolerations: key=node-role.kubernetes.io/master effect=NoSchedule;
    serviceAccountName: calico-kube-controllers;
    priorityClassName: system-cluster-critical;

    container calico-kube-controllers "{{.Repo}}calico/kube-controllers:{{.Version}}" {
        envs {
            ENABLED_CONTROLLERS "node";
            DATASTORE_TYPE "kubernetes";
        }
        readinessProbe {
            exec {
                command: "/usr/bin/check-status" "-r";
            }
        }
    }
}

ServiceAccount:v1 calico-kube-controllers;